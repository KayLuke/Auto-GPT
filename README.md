# Auto-GPT-Turbo

Turbo is the high-octane testing ground for experimental Auto-GPT features. Will these next-gen innovations race into Auto-GPT's lineup or get left in the dust?

[ [Setup Auto-GPT-Turbo](/turbo/docs/setup.md) | [Presets](/turbo/docs/presets.md) ]

## Revving Up Performance
- In gear with the latest stable Auto-GPT (v0.4.7)
- Precision-tuned with Auto-GPT-Benchmarks [link]
- Fueled by your drive â€“ use, test, vote [link]

## The PitStop

What we're testing ...

### A. Presets

> _Customized AI roles, goals & settings for instant execution_

- turbo - The default. Enhanced for speed with one-shot responses and batch command execution.
- codezilla.planner - Transform a simple idea into a development plan - complete with requirements and data structures.
- codezilla.engineer - Evolve your development plan into a fully-integrated codebase.
- kerouac - Unleash your literary instincts and write like a seasoned novelist.
- madison - A haven for creativity and innovation.

Prompts can change the way the LLMs respond and ultimately make them more or less effective. Use presets to speed up your tasks,fulfill specialized tasks, or as examples of Auto-GPT's capabilities.

Presets are pre-configured roles, goals, and prompts that modify the LLM behavior. You can elevate your productivity with the packaged Presets or change them to create your own. The roadmap includes task-level auto-selection, goal-level perset switching, and the ability to save and share presets.

See [Presets](/turbo/docs/presets.md) for more details.

### B. Agent Orchestrator

> _Route jobs to The Best Agent :tm:_

Several impressive AI agent projects exist today, each with unique capabilities. 

Turbo Orchestrator leverages Auto-GPT-Benchmark data to route jobs to the most capable agent. Additionally, you can send your task to multiple agents and rate the results. Planned features include turning goals & results into portable benchmark challenges and feeding the human eval back to Auto-GPT-Benchmarks.

### Other Turbo Features

#### 1. Helicone support
- Enables LLM query caching and monitoring of LLM latency, costs, and benchmarks.

#### 2. Profiler support
- Turbo ships with configurable Wall & CPU profiling of benchmark runs and LLM cycles.

To be continued...
